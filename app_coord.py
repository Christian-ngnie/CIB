# -*- coding: utf-8 -*-
"""App_coord.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/141Kt_6G1DI1RN8TIeRhcJnnueKfoHred
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
import requests
from bs4 import BeautifulSoup
import re
import time
import datetime
import sqlite3
import concurrent.futures
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import DBSCAN
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
import os
import json
import io
import base64
from tqdm import tqdm
import hashlib
from urllib.parse import urlparse
from collections import Counter
import matplotlib.colors as mcolors
from wordcloud import WordCloud
from langdetect import detect
from datetime import timedelta
from PIL import Image
import cv2
import imagehash
from io import BytesIO
import tempfile
try:
    from dateutil import parser
except ImportError:
    pass  # We'll handle this in the parse_date function


# Set page config
st.set_page_config(
    page_title="Coordination and Inauthentic Behavior Detection",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded",
)

# App title and description
st.title("Coordination and Inauthentic Behavior Detection Dashboard")
st.markdown("""
This dashboard analyzes social media data across multiple platforms to detect coordination patterns and inauthentic behavior.
Upload your data with columns (Author, Post, Publication date, URL, Platform) to analyze potential coordinated campaigns.
""")

# Suppress warnings
warnings.filterwarnings('ignore')

# Download NLTK resources
@st.cache_resource
def download_nltk_resources():
    nltk.download('punkt')
    nltk.download('stopwords')

download_nltk_resources()

def standardize_column_names(df):
    """Standardize column names to lowercase and fix any naming inconsistencies"""
    # Create a mapping of original columns to standardized versions
    column_map = {
        'Author': 'author',
        'Post': 'post',
        'Publication date': 'publication_date',  # Changed from 'Publication date' to 'Publication Date'
        'URL': 'url',
        'Platform': 'platform'
    }
    
    # Rename columns that exist in the dataframe
    for old_col, new_col in column_map.items():
        if old_col in df.columns:
            df = df.rename(columns={old_col: new_col})
    
    return df
    
# Initialize database
def init_db():
    conn = sqlite3.connect('coordination_data.db')
    c = conn.cursor()

    # Create content table (main data)
    c.execute('''
    CREATE TABLE IF NOT EXISTS content (
        content_id TEXT PRIMARY KEY,
        author TEXT,
        post TEXT,
        publication_date TEXT,
        timestamp INTEGER,
        url TEXT,
        platform TEXT,
        media_included INTEGER,
        language TEXT,
        last_updated TEXT
    )
    ''')

    # Create analysis results table
    c.execute('''
    CREATE TABLE IF NOT EXISTS analysis_results (
        analysis_id TEXT PRIMARY KEY,
        analysis_type TEXT,
        analysis_date TEXT,
        parameters TEXT,
        results TEXT
    )
    ''')

    # Add indexes for faster queries
    c.execute('CREATE INDEX IF NOT EXISTS idx_content_author ON content(author)')
    c.execute('CREATE INDEX IF NOT EXISTS idx_content_platform ON content(platform)')
    c.execute('CREATE INDEX IF NOT EXISTS idx_content_timestamp ON content(timestamp)')

    conn.commit()
    return conn

# Helper functions
def clean_text(text):
    if not text:
        return ""
    # Convert to string if not already
    text = str(text)
    # Remove HTML tags
    text = re.sub(r'<.*?>', ' ', text)
    # Remove URLs
    text = re.sub(r'https?://\S+|www\.\S+', ' ', text)
    # Remove special characters and numbers
    text = re.sub(r'[^\w\s]', ' ', text)
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def parse_date(date_str):
    """Parse date string to timestamp"""
    try:
        # Make sure date_str is a string
        date_str = str(date_str)

        # Try different date formats
        for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M', '%Y-%m-%d', '%m/%d/%Y %H:%M:%S', '%m/%d/%Y']:
            try:
                dt = datetime.datetime.strptime(date_str, fmt)
                return dt, int(dt.timestamp())
            except ValueError:
                continue

        # If no format matches, try dateutil parser if available
        try:
            from dateutil import parser
            dt = parser.parse(date_str)
            return dt, int(dt.timestamp())
        except ImportError:
            # Fallback if dateutil is not available
            current_time = datetime.datetime.now()
            return current_time, int(current_time.timestamp())
        
    except Exception as e:
        current_time = datetime.datetime.now()
        return current_time, int(current_time.timestamp())


def detect_language(text):
    """Simple language detection"""
    try:
        if not text or len(str(text).strip()) < 3:
            return "unknown"
        
        try:
            from langdetect import detect
            return detect(str(text))
        except ImportError:
            # Fallback if langdetect is not available
            return "en"  # Default to English
            
    except:
        # If detection fails, make a simple guess
        text = str(text)
        words = text.split()
        avg_word_length = sum(len(word) for word in words) / len(words) if words else 0
        spaces_ratio = text.count(' ') / len(text) if len(text) > 0 else 0

        if spaces_ratio > 0.1:
            return "en"  # Likely English or similar
        else:
            return "unknown"


def detect_media_references(text, url):
    """Detect references to media (images, videos) in text or URL"""
    # Make sure inputs are strings
    text = str(text) if text is not None else ""
    url = str(url) if url is not None else ""

    # Check for common image/video extensions or platforms in URL
    media_patterns = [
        r'\.jpg|\.jpeg|\.png|\.gif|\.bmp|\.webp',
        r'\.mp4|\.avi|\.mov|\.wmv|\.flv',
        r'youtube\.com|youtu\.be|vimeo\.com|tiktok\.com|instagram\.com'
    ]

    # Check URL for media references
    if url:
        for pattern in media_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return 1

    # Check text for media references
    media_terms = [
        r'photo', r'image', r'picture', r'pic', r'meme',
        r'video', r'watch', r'look at', r'youtube', r'instagram',
        r'tiktok', r'graphic', r'infographic', r'screenshot'
    ]

    for term in media_terms:
        if re.search(r'\b' + term + r'\b', text, re.IGNORECASE):
            return 1

    return 0

def calculate_content_hash(text):
    """Create a hash of text content to identify duplicates/near-duplicates"""
    if not text:
        return ""

    # Convert to string if not already
    text = str(text)

    # Normalize text: lowercase, remove punctuation and extra spaces
    normalized = re.sub(r'[^\w\s]', '', text.lower())
    normalized = re.sub(r'\s+', ' ', normalized).strip()

    # Create MD5 hash
    return hashlib.md5(normalized.encode()).hexdigest()

def extract_domain(url):
    """Extract domain from URL"""
    if not url:
        return ""

    # Convert to string if not already
    url = str(url)

    try:
        parsed_url = urlparse(url)
        domain = parsed_url.netloc
        return domain
    except:
        return ""

def save_to_database(data, conn):
    cursor = conn.cursor()

    # Current time for last_updated
    now = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    # Process each row of data
    for _, row in data.iterrows():
        # Generate a unique content_id
        content_id = hashlib.md5((str(row['author']) + str(row['url']) + str(row['publication_date'])).encode()).hexdigest()

        # Parse date and get timestamp
        _, timestamp = parse_date(str(row['publication_date']))

        # Detect if media is included
        media_included = detect_media_references(str(row['post']), str(row['url']))

        # Detect language
        language = detect_language(str(row['post']))

        # Insert data
        cursor.execute('''
        INSERT OR REPLACE INTO content
        (content_id, author, post, publication_date, timestamp, url, platform, media_included, language, last_updated)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            content_id,
            str(row['author']),
            str(row['post']),
            str(row['publication_date']),
            timestamp,
            str(row['url']),
            str(row['platform']),
            media_included,
            language,
            now
        ))

    conn.commit()

def load_from_database(conn, filters=None):
    """Load data from database with optional filters"""
    query = "SELECT * FROM content"
    params = []

    if filters:
        where_clauses = []

        if 'platform' in filters and filters['platform']:
            where_clauses.append("platform = ?")
            params.append(filters['platform'])

        if 'author' in filters and filters['author']:
            where_clauses.append("author = ?")
            params.append(filters['author'])

        if 'start_date' in filters and filters['start_date']:
            start_timestamp = int(filters['start_date'].timestamp())
            where_clauses.append("timestamp >= ?")
            params.append(start_timestamp)

        if 'end_date' in filters and filters['end_date']:
            end_timestamp = int(filters['end_date'].timestamp())
            where_clauses.append("timestamp <= ?")
            params.append(end_timestamp)

        if 'media_only' in filters and filters['media_only']:
            where_clauses.append("media_included = 1")

        if 'language' in filters and filters['language']:
            where_clauses.append("language = ?")
            params.append(filters['language'])

        if where_clauses:
            query += " WHERE " + " AND ".join(where_clauses)

    df = pd.read_sql_query(query, conn, params=params)

    # Convert timestamp to datetime for easier handling
    if 'timestamp' in df.columns:
        df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')

    return df


# Analysis functions
def analyze_platform_distribution(df):
    """Analyze the distribution of content across platforms"""
    # Group by platform
    platform_counts = df['platform'].value_counts().reset_index()
    platform_counts.columns = ['Platform', 'Count']

    # Calculate percentage
    total = platform_counts['Count'].sum()
    platform_counts['Percentage'] = (platform_counts['Count'] / total * 100).round(2)

    # Group by platform and author
    platform_authors = df.groupby('platform')['author'].nunique().reset_index()
    platform_authors.columns = ['Platform', 'Unique Authors']

    # Merge the two dataframes
    platform_stats = platform_counts.merge(platform_authors, on='Platform')

    return platform_stats


def analyze_media_usage(df):
    """Analyze media usage patterns"""
    # Count content with media references
    media_counts = df['media_included'].value_counts().reset_index()
    media_counts.columns = ['Media Included', 'Count']

    # Calculate media usage by platform
    platform_media = df.groupby('platform')['media_included'].sum().reset_index()
    platform_media.columns = ['Platform', 'Media Count']

    # Calculate percentage of content with media by platform
    platform_media_pct = df.groupby('platform').agg(
        total_posts=('media_included', 'count'),
        media_posts=('media_included', 'sum')
    ).reset_index()
    platform_media_pct['Percentage'] = (platform_media_pct['media_posts'] / platform_media_pct['total_posts'] * 100).round(2)

    # Calculate media usage by author
    author_media = df.groupby('author')['media_included'].agg(['sum', 'count']).reset_index()
    author_media.columns = ['Author', 'Media Count', 'Total Posts']
    author_media['Media Percentage'] = (author_media['Media Count'] / author_media['Total Posts'] * 100).round(2)
    author_media = author_media.sort_values('Media Percentage', ascending=False)

    return {
        'overall': media_counts,
        'by_platform': platform_media_pct,
        'by_author': author_media
    }

def analyze_posting_frequency(df):
    """Analyze posting frequency patterns"""
    # Convert timestamp to datetime
    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')

    # Extract date
    df['date'] = df['datetime'].dt.date

    # Group by author and date, count posts
    freq_df = df.groupby(['author', 'date']).size().reset_index(name='post_count')

    # Get daily average per author
    avg_posts = freq_df.groupby('author')['post_count'].mean().reset_index()
    avg_posts.columns = ['author', 'avg_daily_posts']

    # Get posting days per author
    days_posted = freq_df.groupby('author').size().reset_index(name='days_posted')

    # Get total posts per author
    total_posts = df.groupby('author').size().reset_index(name='total_posts')

    # Get first and last post dates
    first_post = df.groupby('author')['datetime'].min().reset_index()
    first_post.columns = ['author', 'first_post']

    last_post = df.groupby('author')['datetime'].max().reset_index()
    last_post.columns = ['author', 'last_post']

    # Merge all statistics
    freq_stats = avg_posts.merge(days_posted, on='author') \
                          .merge(total_posts, on='author') \
                          .merge(first_post, on='author') \
                          .merge(last_post, on='author')

    # Calculate activity duration in days
    freq_stats['duration_days'] = (freq_stats['last_post'] - freq_stats['first_post']).dt.days + 1

    # Calculate posts per day (over entire duration)
    freq_stats['posts_per_day'] = (freq_stats['total_posts'] / freq_stats['duration_days']).round(2)

    # Calculate activity rate (percentage of days active)
    freq_stats['activity_rate'] = (freq_stats['days_posted'] / freq_stats['duration_days'] * 100).round(2)

    return freq_stats

def analyze_time_patterns(df):
    """Analyze posting time patterns"""
    # Convert timestamp to datetime
    #df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')

    # Extract hour, day of week, and day of month
    df['hour'] = df['datetime'].dt.hour
    df['day_of_week'] = df['datetime'].dt.dayofweek
    df['day_name'] = df['datetime'].dt.day_name()
    df['day_of_month'] = df['datetime'].dt.day

    # Count posts by hour and author
    hour_counts = df.groupby(['author', 'hour']).size().reset_index(name='post_count')

    # Count posts by day of week and author
    day_counts = df.groupby(['author', 'day_of_week', 'day_name']).size().reset_index(name='post_count')
    day_counts = day_counts.sort_values(['author', 'day_of_week'])

    # Get hour with most posts for each author
    peak_hours = hour_counts.loc[hour_counts.groupby('author')['post_count'].idxmax()]
    peak_hours.columns = ['Author', 'Peak Hour', 'Count']

    # Get day with most posts for each author
    peak_days = day_counts.loc[day_counts.groupby('author')['post_count'].idxmax()]
    peak_days = peak_days[['author', 'day_name', 'post_count']]
    peak_days.columns = ['Author', 'Peak Day', 'Count']

    return hour_counts, day_counts, peak_hours, peak_days

def analyze_content_similarity(df):
    """Analyze content similarity between authors"""
    # Get unique authors
    authors = df['author'].unique()

    # Combine all posts for each author
    author_texts = {}
    for author in authors:
        author_posts = df[df['author'] == author]['post'].tolist()
        author_texts[author] = ' '.join([str(post) for post in author_posts])

    # Create TF-IDF vectors
    tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')

    try:
        tfidf_matrix = tfidf_vectorizer.fit_transform(list(author_texts.values()))

        # Calculate cosine similarity
        similarity_matrix = cosine_similarity(tfidf_matrix)

        # Create DataFrame with similarity scores
        similarity_df = pd.DataFrame(similarity_matrix, index=authors, columns=authors)

        return similarity_df
    except:
        # Return empty dataframe if there's an error (e.g., no text content)
        return pd.DataFrame(index=authors, columns=authors)

def analyze_url_patterns(df):
    """Analyze URL sharing patterns"""
    # Extract domains from URLs
    df['domain'] = df['url'].apply(extract_domain)

    # Count domains overall
    domain_counts = df['domain'].value_counts().reset_index()
    domain_counts.columns = ['Domain', 'Count']

    # Count domains by author
    author_domains = df.groupby(['author', 'domain']).size().reset_index(name='count')
    author_domains = author_domains.sort_values(['author', 'count'], ascending=[True, False])

    # Find shared URLs
    url_counts = df['url'].value_counts().reset_index()
    url_counts.columns = ['URL', 'Count']
    shared_urls = url_counts[url_counts['Count'] > 1]

    # For shared URLs, find which authors shared them
    shared_url_authors = []
    for url in shared_urls['URL']:
        authors = df[df['url'] == url]['author'].unique().tolist()
        shared_url_authors.append({
            'URL': url,
            'Count': len(authors),
            'Authors': authors
        })

    shared_url_df = pd.DataFrame(shared_url_authors)

    return {
        'domain_counts': domain_counts,
        'author_domains': author_domains,
        'shared_urls': shared_url_df if not shared_url_df.empty else pd.DataFrame()
    }

def detect_temporal_patterns(df):
    """Detect temporal coordination patterns"""
    # Define a window size (e.g., 30 minutes) to identify posts in close time proximity
    window_size = 30 * 60  # 30 minutes in seconds

    # Create a copy with datetime
    temp_df = df.copy()
    #temp_df['datetime'] = pd.to_datetime(temp_df['timestamp'], unit='s')

    # Sort by timestamp
    temp_df = temp_df.sort_values('timestamp')

    # Create an empty DataFrame for results
    columns = ['window_start', 'window_end', 'platform', 'authors', 'post_count', 'similar_content']
    coordination_windows = pd.DataFrame(columns=columns)

    # Identify potential coordination windows
    for platform in temp_df['platform'].unique():
        platform_posts = temp_df[temp_df['platform'] == platform]

        # Skip platforms with too few posts
        if len(platform_posts) <= 5:
            continue

        for i, row in platform_posts.iterrows():
            timestamp = row['timestamp']
            start_time = timestamp - window_size/2
            end_time = timestamp + window_size/2

            # Find posts within the time window
            window_posts = platform_posts[
                (platform_posts['timestamp'] >= start_time) &
                (platform_posts['timestamp'] <= end_time)
            ]

            # If multiple authors posted in this window
            if len(window_posts['author'].unique()) > 1:
                # Check for similar content
                similar_content = False
                if len(window_posts) > 1:
                    posts = window_posts['post'].tolist()
                    if len(posts) > 1:
                        # Simple check for similarity - will be refined in a more advanced version
                        tfidf = TfidfVectorizer().fit_transform([str(post) for post in posts])
                        pairwise_sim = cosine_similarity(tfidf)

                        # If any pair of posts is similar enough
                        if np.max(pairwise_sim - np.eye(len(posts))) > 0.3:
                            similar_content = True

                # Add to results
                new_row = {
                    'window_start': pd.to_datetime(start_time, unit='s'),
                    'window_end': pd.to_datetime(end_time, unit='s'),
                    'platform': platform,
                    'authors': list(window_posts['author'].unique()),
                    'post_count': len(window_posts),
                    'similar_content': similar_content
                }

                coordination_windows = pd.concat([coordination_windows, pd.DataFrame([new_row])], ignore_index=True)

    # Remove overlapping windows
    if not coordination_windows.empty:
        coordination_windows = coordination_windows.sort_values('post_count', ascending=False)
        filtered_windows = []
        used_timestamps = set()

        for _, window in coordination_windows.iterrows():
            start = window['window_start'].timestamp()
            end = window['window_end'].timestamp()
            timestamp_range = set(range(int(start), int(end), 60))  # 1-minute intervals

            # Check if this window overlaps significantly with prior windows
            overlap = len(timestamp_range.intersection(used_timestamps)) / len(timestamp_range) if timestamp_range else 0

            if overlap < 0.5:  # Less than 50% overlap
                filtered_windows.append(window)
                used_timestamps.update(timestamp_range)

        coordination_windows = pd.DataFrame(filtered_windows)

    return coordination_windows

def analyze_language_patterns(df):
    """Analyze language patterns to detect targeting"""
    # Count posts by language
    language_counts = df['language'].value_counts().reset_index()
    language_counts.columns = ['Language', 'Count']

    # Calculate language usage by author
    author_languages = df.groupby(['author', 'language']).size().reset_index(name='count')
    author_languages = author_languages.sort_values(['author', 'count'], ascending=[True, False])

    # Calculate language usage by platform
    platform_languages = df.groupby(['platform', 'language']).size().reset_index(name='count')
    platform_languages = platform_languages.sort_values(['platform', 'count'], ascending=[True, False])

    return {
        'overall': language_counts,
        'by_author': author_languages,
        'by_platform': platform_languages
    }

def detect_coordination_pattern_types(df, temporal_windows):
    """Classify coordination patterns by type (disruption, suppression, amplification)"""
    # Create empty results
    pattern_types = {
        'amplification': [],
        'disruption': [],
        'suppression': [],
        'unknown': []
    }

    # If we have temporal coordination data
    if not temporal_windows.empty:
        # Analyze each coordination window
        for _, window in temporal_windows.iterrows():
            authors = window['authors']
            platform = window['platform']
            start_time = window['window_start']
            end_time = window['window_end']

            # Get posts in this window
            window_posts = df[
                (df['platform'] == platform) &
                (df['author'].isin(authors)) &
                (pd.to_datetime(df['timestamp'], unit='s') >= start_time) &
                (pd.to_datetime(df['timestamp'], unit='s') <= end_time)
            ]

            # Analyze content for pattern type
            posts = window_posts['post'].tolist()

            # Simple keyword-based classification (will be refined in a more advanced version)
            amplification_keywords = ['support', 'agree', 'like', 'share', 'retweet', 'amplify', 'spread']
            disruption_keywords = ['false', 'fake', 'lie', 'wrong', 'corrupt', 'against', 'protest']
            suppression_keywords = ['don\'t', 'stop', 'avoid', 'ignore', 'ban', 'prevent', 'block']

            # Count keyword matches
            amp_count = 0
            dis_count = 0
            sup_count = 0

            for post in posts:
                post_text = str(post).lower()

                # Count matches
                amp_count += sum(1 for keyword in amplification_keywords if keyword in post_text)
                dis_count += sum(1 for keyword in disruption_keywords if keyword in post_text)
                sup_count += sum(1 for keyword in suppression_keywords if keyword in post_text)

            # Classify based on keyword counts
            if amp_count > dis_count and amp_count > sup_count and amp_count > 0:
                pattern_types['amplification'].append({
                    'window_start': start_time,
                    'platform': platform,
                    'authors': authors,
                    'post_count': len(window_posts)
                })
            elif dis_count > amp_count and dis_count > sup_count and dis_count > 0:
                pattern_types['disruption'].append({
                    'window_start': start_time,
                    'platform': platform,
                    'authors': authors,
                    'post_count': len(window_posts)
                })
            elif sup_count > amp_count and sup_count > dis_count and sup_count > 0:
                pattern_types['suppression'].append({
                    'window_start': start_time,
                    'platform': platform,
                    'authors': authors,
                    'post_count': len(window_posts)
                })
            else:
                pattern_types['unknown'].append({
                    'window_start': start_time,
                    'platform': platform,
                    'authors': authors,
                    'post_count': len(window_posts)
                })

    # Convert lists to DataFrames
    for pattern_type in pattern_types:
        if pattern_types[pattern_type]:
            pattern_types[pattern_type] = pd.DataFrame(pattern_types[pattern_type])
        else:
            pattern_types[pattern_type] = pd.DataFrame(columns=['window_start', 'platform', 'authors', 'post_count'])

    return pattern_types

def identify_coordination_networks(df, content_sim_df, temporal_windows, url_patterns):
    """Identify networks of coordinated accounts"""
    # Get unique authors
    authors = df['author'].unique()

    # Create a matrix for coordination scores
    coord_scores = pd.DataFrame(0, index=authors, columns=authors)

    # Factor 1: Content similarity (scaled 0-5)
    for author1 in authors:
        for author2 in authors:
            if author1 != author2 and author1 in content_sim_df.index and author2 in content_sim_df.columns:
                # Content similarity contributes 0-5 to the score
                sim_score = content_sim_df.loc[author1, author2]
                if not pd.isna(sim_score):
                    content_sim = sim_score * 5
                    coord_scores.loc[author1, author2] += content_sim

    # Factor 2: Temporal coordination (each instance adds 2 points)
    if not temporal_windows.empty:
        for _, window in temporal_windows.iterrows():
            authors_list = window['authors']
            for i, author1 in enumerate(authors_list):
                for author2 in authors_list[i+1:]:
                    if author1 in coord_scores.index and author2 in coord_scores.columns:
                        coord_scores.loc[author1, author2] += 2
                        coord_scores.loc[author2, author1] += 2

    # Factor 3: Shared URLs (each shared URL adds 1 point)
    if 'shared_urls' in url_patterns and not url_patterns['shared_urls'].empty:
        for _, row in url_patterns['shared_urls'].iterrows():
            authors_list = row['Authors']
            for i, author1 in enumerate(authors_list):
                for author2 in authors_list[i+1:]:
                    if author1 in coord_scores.index and author2 in coord_scores.columns:
                        coord_scores.loc[author1, author2] += 1
                        coord_scores.loc[author2, author1] += 1

    # Create network graph
    G = nx.Graph()

    # Add nodes (authors)
    for author in authors:
        G.add_node(author)

    # Add edges with weights based on coordination scores
    threshold = 3  # Minimum score to consider coordination
    for author1 in authors:
        for author2 in authors:
            if author1 != author2:
                score = coord_scores.loc[author1, author2]
                if score > threshold:
                    G.add_edge(author1, author2, weight=score)

    # Find connected components (clusters)
    clusters = list(nx.connected_components(G))

    # Format results
    cluster_results = []
    for i, cluster in enumerate(clusters):
        if len(cluster) > 1:  # Only include clusters with more than one author
            cluster_results.append({
                'cluster_id': i + 1,
                'members': list(cluster),
                'size': len(cluster),
                'density': nx.density(G.subgraph(cluster)) if len(cluster) > 1 else 0
            })

    # Calculate network metrics
    network_metrics = {
        'total_authors': len(authors),
        'coordinated_authors': sum(len(cluster) for cluster in clusters if len(cluster) > 1),
        'total_clusters': len([c for c in clusters if len(c) > 1]),
        'largest_cluster_size': max([len(c) for c in clusters]) if clusters else 0,
        'average_cluster_size': sum(len(c) for c in clusters) / len(clusters) if clusters else 0,
        'graph_density': nx.density(G)
    }

    return {
        'clusters': cluster_results,
        'metrics': network_metrics,
        'graph': G,
        'scores': coord_scores
    }
def analyze_topic_patterns(df):
    """Analyze topic patterns using TF-IDF to identify targeted topics"""
    # Clean posts for topic analysis
    df['clean_post'] = df['post'].apply(clean_text)

    # Skip empty posts
    df_topics = df[df['clean_post'].str.len() > 10].copy()

    if len(df_topics) < 5:
        return {'top_topics': pd.DataFrame(), 'author_topics': pd.DataFrame(), 'platform_topics': pd.DataFrame()}

    # Vectorize text
    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1, 2))
    tfidf_matrix = vectorizer.fit_transform(df_topics['clean_post'])

    # Get feature names
    feature_names = vectorizer.get_feature_names_out()

    # Get top terms overall
    importance = np.asarray(tfidf_matrix.mean(axis=0)).flatten()
    indices = importance.argsort()[-20:][::-1]
    top_terms = [(feature_names[i], importance[i]) for i in indices]
    top_topics_df = pd.DataFrame(top_terms, columns=['Topic', 'Importance'])

    # Get top terms by author
    author_topics = {}
    for author in df_topics['author'].unique():
        author_posts = df_topics[df_topics['author'] == author]
        if len(author_posts) >= 3:  # Minimum posts for topic analysis
            author_tfidf = vectorizer.transform(author_posts['clean_post'])
            author_importance = np.asarray(author_tfidf.mean(axis=0)).flatten()
            author_indices = author_importance.argsort()[-10:][::-1]
            author_topics[author] = [(feature_names[i], author_importance[i]) for i in author_indices]

    # Convert author topics to DataFrame
    author_topics_rows = []
    for author, topics in author_topics.items():
        for topic, importance in topics:
            author_topics_rows.append({'Author': author, 'Topic': topic, 'Importance': importance})
    author_topics_df = pd.DataFrame(author_topics_rows)

    # Get top terms by platform
    platform_topics = {}
    for platform in df_topics['platform'].unique():
        platform_posts = df_topics[df_topics['platform'] == platform]
        if len(platform_posts) >= 3:  # Minimum posts for topic analysis
            platform_tfidf = vectorizer.transform(platform_posts['clean_post'])
            platform_importance = np.asarray(platform_tfidf.mean(axis=0)).flatten()
            platform_indices = platform_importance.argsort()[-10:][::-1]
            platform_topics[platform] = [(feature_names[i], platform_importance[i]) for i in platform_indices]

    # Convert platform topics to DataFrame
    platform_topics_rows = []
    for platform, topics in platform_topics.items():
        for topic, importance in topics:
            platform_topics_rows.append({'Platform': platform, 'Topic': topic, 'Importance': importance})
    platform_topics_df = pd.DataFrame(platform_topics_rows)

    return {
        'top_topics': top_topics_df,
        'author_topics': author_topics_df,
        'platform_topics': platform_topics_df
    }

def analyze_image_sharing(df):
    """Analyze patterns in image sharing"""
    # Filter posts with media
    media_posts = df[df['media_included'] == 1].copy()

    if len(media_posts) == 0:
        return {'media_counts': pd.DataFrame(), 'author_media': pd.DataFrame(), 'platform_media': pd.DataFrame()}

    # Count media posts by author
    author_media = media_posts.groupby('author').size().reset_index(name='media_count')

    # Calculate percentage of posts with media for each author
    author_total = df.groupby('author').size().reset_index(name='total_posts')
    author_media = author_media.merge(author_total, on='author')
    author_media['media_percentage'] = (author_media['media_count'] / author_media['total_posts'] * 100).round(2)
    author_media = author_media.sort_values('media_percentage', ascending=False)

    # Count media posts by platform
    platform_media = media_posts.groupby('platform').size().reset_index(name='media_count')

    # Calculate percentage of posts with media for each platform
    platform_total = df.groupby('platform').size().reset_index(name='total_posts')
    platform_media = platform_media.merge(platform_total, on='platform')
    platform_media['media_percentage'] = (platform_media['media_count'] / platform_media['total_posts'] * 100).round(2)
    platform_media = platform_media.sort_values('media_percentage', ascending=False)

    # Count total media posts
    media_counts = pd.DataFrame([
        {'Type': 'Media Posts', 'Count': len(media_posts)},
        {'Type': 'Text-only Posts', 'Count': len(df) - len(media_posts)}
    ])

    return {
        'media_counts': media_counts,
        'author_media': author_media,
        'platform_media': platform_media
    }

def detect_meme_sharing(df, image_urls=None):
    """Detect and analyze meme sharing patterns"""
    # This is a placeholder for actual meme detection, which would require image analysis
    # In a real implementation, you would download the images and analyze them

    # Extract image URLs from df['url'] that contain image extensions
    image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp']
    potential_memes = []

    if image_urls is None:
        image_urls = []
        for _, row in df.iterrows():
            url = str(row['url'])
            if any(ext in url.lower() for ext in image_extensions):
                image_urls.append({
                    'url': url,
                    'author': row['author'],
                    'platform': row['platform'],
                    'timestamp': row['timestamp']
                })

    # Simple meme detection based on URL keywords (basic approach)
    meme_keywords = ['meme', 'funny', 'lol', 'joke', 'humor']

    for img_data in image_urls:
        url = img_data['url'].lower()
        # Check if URL contains meme keywords
        if any(keyword in url for keyword in meme_keywords):
            potential_memes.append({
                'url': img_data['url'],
                'author': img_data['author'],
                'platform': img_data['platform'],
                'timestamp': img_data['timestamp'],
                'confidence': 'medium'
            })

    # Count memes by author
    meme_authors = {}
    for meme in potential_memes:
        author = meme['author']
        if author in meme_authors:
            meme_authors[author] += 1
        else:
            meme_authors[author] = 1

    # Create meme sharing metrics
    meme_metrics = {
        'total_potential_memes': len(potential_memes),
        'meme_sharing_authors': len(meme_authors),
        'top_meme_sharers': sorted(meme_authors.items(), key=lambda x: x[1], reverse=True)[:10],
        'memes': potential_memes
    }

    return meme_metrics

def detect_audience_targeting(df, topic_results, language_results, time_patterns):
    """Detect potential audience targeting based on language, topics, and timing"""
    targeting_indicators = []

    # 1. Language-based targeting
    if not language_results['overall'].empty:
        # Check if multiple languages are used, suggesting targeting different linguistic groups
        if len(language_results['overall']) > 1:
            dominant_languages = language_results['overall'].head(3)['Language'].tolist()
            targeting_indicators.append({
                'type': 'language',
                'description': f"Multiple languages detected ({', '.join(dominant_languages)}), suggesting targeting of different linguistic groups.",
                'evidence': language_results['overall'].to_dict('records'),
                'confidence': 'medium'
            })

    # 2. Topic-based targeting
    if 'platform_topics' in topic_results and not topic_results['platform_topics'].empty:
        # Look for sensitive or polarizing topics
        sensitive_topics = ['election', 'vote', 'immigrant', 'refugee', 'government', 'scandal',
                           'protest', 'conspiracy', 'crisis', 'climate', 'vaccine', 'pandemic']

        found_topics = []
        for _, row in topic_results['platform_topics'].iterrows():
            if any(topic in row['Topic'].lower() for topic in sensitive_topics):
                found_topics.append(row['Topic'])

        if found_topics:
            targeting_indicators.append({
                'type': 'topic',
                'description': f"Potentially divisive topics detected: {', '.join(found_topics)}",
                'evidence': found_topics,
                'confidence': 'medium'
            })

    # 3. Timing-based targeting
    hour_counts, day_counts, peak_hours, peak_days = time_patterns

    # Check for posts during specific time windows that might target certain demographics
    # Simplified: check if posts are concentrated in evening hours (when people are home from work)
    evening_hours = [18, 19, 20, 21, 22]
    evening_posts = hour_counts[hour_counts['hour'].isin(evening_hours)]

    if not evening_posts.empty:
        evening_percentage = (evening_posts['post_count'].sum() / hour_counts['post_count'].sum()) * 100

        if evening_percentage > 50:  # If more than 50% of posts are in evening hours
            targeting_indicators.append({
                'type': 'timing',
                'description': f"Posts concentrated in evening hours ({evening_percentage:.1f}% of posts), suggesting targeting of after-work audience.",
                'evidence': {'evening_percentage': evening_percentage},
                'confidence': 'medium'
            })

    # Check for weekend targeting
    weekend_days = ['Saturday', 'Sunday']
    weekend_posts = day_counts[day_counts['day_name'].isin(weekend_days)]

    if not weekend_posts.empty:
        weekend_percentage = (weekend_posts['post_count'].sum() / day_counts['post_count'].sum()) * 100

        if weekend_percentage > 40:  # If more than 40% of posts are on weekends
            targeting_indicators.append({
                'type': 'timing',
                'description': f"High weekend activity ({weekend_percentage:.1f}% of posts), suggesting targeting of weekend leisure audience.",
                'evidence': {'weekend_percentage': weekend_percentage},
                'confidence': 'medium'
            })

    return targeting_indicators

def estimate_operation_scale(network_results, temporal_patterns, url_patterns):
    """Estimate the scale of the coordinated operation"""
    # Default scale assessment
    scale_assessment = {
        'scale': 'unknown',
        'confidence': 'low',
        'description': "Insufficient data to determine operation scale.",
        'metrics': {}
    }

    # Extract metrics from network results
    if 'metrics' in network_results:
        metrics = network_results['metrics']

        # Determine scale based on number of coordinated authors and clusters
        coordinated_authors = metrics.get('coordinated_authors', 0)
        total_clusters = metrics.get('total_clusters', 0)
        largest_cluster = metrics.get('largest_cluster_size', 0)

        # Calculate additional metrics
        coordination_density = 0
        if 'graph' in network_results:
            G = network_results['graph']
            if G.number_of_nodes() > 0:
                coordination_density = nx.density(G)

        # Criteria for scale determination
        scale_metrics = {
            'coordinated_authors': coordinated_authors,
            'total_clusters': total_clusters,
            'largest_cluster_size': largest_cluster,
            'coordination_density': coordination_density
        }

        # Scale determination logic
        if coordinated_authors >= 20 and total_clusters >= 3 and largest_cluster >= 10:
            scale = 'large'
            confidence = 'high'
            description = "Large-scale coordinated network with multiple clusters and high author count."
        elif coordinated_authors >= 10 and total_clusters >= 2:
            scale = 'medium'
            confidence = 'medium'
            description = "Medium-scale coordination network with multiple connected author clusters."
        elif coordinated_authors >= 3:
            scale = 'small'
            confidence = 'medium'
            description = "Small-scale coordination with a few connected authors."
        elif coordinated_authors > 0:
            scale = 'minimal'
            confidence = 'medium'
            description = "Minimal coordination detected between very few authors."
        else:
            scale = 'none'
            confidence = 'high'
            description = "No evidence of coordinated behavior between authors."

        scale_assessment = {
            'scale': scale,
            'confidence': confidence,
            'description': description,
            'metrics': scale_metrics
        }

    return scale_assessment

def generate_network_visualization(network_results):
    """Generate a visual representation of the coordination network"""
    if 'graph' not in network_results:
        return None

    G = network_results['graph']

    if G.number_of_nodes() == 0:
        return None

    # Create positions for the nodes
    pos = nx.spring_layout(G, seed=42)

    # Create a Plotly figure
    fig = go.Figure()

    # Add edges
    edge_x = []
    edge_y = []
    edge_weights = []

    for edge in G.edges(data=True):
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_x.extend([x0, x1, None])
        edge_y.extend([y0, y1, None])
        weight = edge[2].get('weight', 1)
        edge_weights.append(weight)

    # Scale edge width by weight
    max_weight = max(edge_weights) if edge_weights else 1
    edge_weights_scaled = [w/max_weight * 5 for w in edge_weights]

    # Add the edges to the plot
    fig.add_trace(go.Scatter(
        x=edge_x, y=edge_y,
        line=dict(width=2, color='rgba(150,150,150,0.8)'),
        hoverinfo='none',
        mode='lines'
    ))

    # Add nodes
    # Generate clustering for coloring
    if len(G.nodes) >= 3:
        try:
            communities = nx.algorithms.community.greedy_modularity_communities(G)
            community_map = {}
            for i, community in enumerate(communities):
                for node in community:
                    community_map[node] = i
        except:
            community_map = {node: 0 for node in G.nodes}
    else:
        community_map = {node: 0 for node in G.nodes}

    # Get node data
    node_x = []
    node_y = []
    node_text = []
    node_size = []
    node_color = []

    for node in G.nodes():
        x, y = pos[node]
        node_x.append(x)
        node_y.append(y)
        node_text.append(f"Author: {node}<br>Connections: {G.degree(node)}")
        node_size.append(G.degree(node) * 5 + 10)  # Size based on number of connections
        node_color.append(community_map.get(node, 0))  # Color based on community

    # Add the nodes to the plot
    fig.add_trace(go.Scatter(
        x=node_x, y=node_y,
        mode='markers',
        marker=dict(
            size=node_size,
            color=node_color,
            colorscale='Viridis',
            line=dict(width=1, color='rgba(50,50,50,0.8)')
        ),
        text=node_text,
        hoverinfo='text'
    ))

    # Update the layout
    fig.update_layout(
        title='Coordination Network',
        showlegend=False,
        hovermode='closest',
        margin=dict(b=20, l=5, r=5, t=40),
        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        width=800,
        height=600
    )

    return fig

def generate_timeline_visualization(df, temporal_patterns):
    """Generate a timeline visualization of posting activity and coordination windows"""
    if df.empty or temporal_patterns.empty:
        return None

    # Create a figure with secondary y-axis
    fig = make_subplots(specs=[[{"secondary_y": True}]])

    # Post counts by date
    df['date'] = pd.to_datetime(df['timestamp'], unit='s').dt.date
    daily_posts = df.groupby('date').size().reset_index(name='post_count')
    daily_posts['date'] = pd.to_datetime(daily_posts['date'])

    # Add the post counts line
    fig.add_trace(
        go.Scatter(
            x=daily_posts['date'],
            y=daily_posts['post_count'],
            name="Posts per Day",
            mode='lines+markers',
            line=dict(color='blue', width=2)
        ),
        secondary_y=False,
    )

    # Add coordination windows as markers
    if not temporal_patterns.empty:
        window_dates = temporal_patterns['window_start'].dt.date.unique()
        window_dates = pd.to_datetime(window_dates)

        window_counts = temporal_patterns.groupby(temporal_patterns['window_start'].dt.date).size().reset_index(name='window_count')
        window_counts['window_start'] = pd.to_datetime(window_counts['window_start'])

        fig.add_trace(
            go.Scatter(
                x=window_counts['window_start'],
                y=window_counts['window_count'],
                name="Coordination Windows",
                mode='markers',
                marker=dict(
                    color='red',
                    size=window_counts['window_count'] * 5 + 5,
                    line=dict(width=1, color='darkred')
                )
            ),
            secondary_y=True,
        )

    # Update layout
    fig.update_layout(
        title_text="Posting Activity and Detected Coordination Timeline",
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        ),
        xaxis=dict(title="Date"),
        hovermode="x unified"
    )

    fig.update_yaxes(title_text="Posts per Day", secondary_y=False)
    fig.update_yaxes(title_text="Coordination Windows", secondary_y=True)

    return fig

# Dashboard UI components
def upload_data_component():
    """UI component for data upload"""
    st.header("Data Upload and Processing")

    uploaded_file = st.file_uploader("Upload your data (CSV or Excel file)", type=["csv", "xlsx"])

    if uploaded_file is not None:
        try:
            # Determine file type and read data
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            elif uploaded_file.name.endswith('.xlsx'):
                df = pd.read_excel(uploaded_file)

            # Standardize column names to lowercase
            df = standardize_column_names(df)

            # Check required columns (now using lowercase names)
            required_columns = ['author', 'post', 'publication_date', 'url', 'platform']
            missing_columns = [col for col in required_columns if col not in df.columns]

            if missing_columns:
                st.error(f"Missing required columns: {', '.join(missing_columns)}")
                return None

            # Display data preview
            st.subheader("Data Preview")
            st.dataframe(df.head())

            # Display data summary
            st.subheader("Data Summary")
            col1, col2, col3 = st.columns(3)
            col1.metric("Total Posts", len(df))
            col2.metric("Unique Authors", df['author'].nunique())
            col3.metric("Platforms", df['platform'].nunique())

            # Initialize database and save data
            conn = init_db()
            save_to_database(df, conn)

            st.success(f"Data successfully processed and saved to database. Total records: {len(df)}")

            return conn

        except Exception as e:
            st.error(f"Error processing file: {str(e)}")
            return None

    # If no file is uploaded, check if database exists
    if os.path.exists('coordination_data.db'):
        st.info("Using existing database.")
        return sqlite3.connect('coordination_data.db')

    return None

def filter_options_component(conn):
    """UI component for data filtering options"""
    st.sidebar.header("Filter Options")

    if conn is None:
        st.sidebar.warning("Please upload data first.")
        return None

    # Get available platforms
    platforms_df = pd.read_sql("SELECT DISTINCT platform FROM content", conn)
    platforms = platforms_df['platform'].tolist()

    # Get available languages
    languages_df = pd.read_sql("SELECT DISTINCT language FROM content", conn)
    languages = languages_df['language'].tolist()

    # Get min and max dates
    date_df = pd.read_sql("SELECT MIN(timestamp) as min_timestamp, MAX(timestamp) as max_timestamp FROM content", conn)
    min_date = pd.to_datetime(date_df['min_timestamp'].iloc[0], unit='s').date() if not date_df.empty else datetime.date.today()
    max_date = pd.to_datetime(date_df['max_timestamp'].iloc[0], unit='s').date() if not date_df.empty else datetime.date.today()

    # Create filters
    selected_platform = st.sidebar.selectbox("Platform", ["All"] + platforms)

    selected_language = st.sidebar.selectbox("Language", ["All"] + languages)

    date_range = st.sidebar.date_input(
        "Date Range",
        value=(min_date, max_date),
        min_value=min_date,
        max_value=max_date
    )

    if len(date_range) == 2:
        start_date, end_date = date_range
    else:
        start_date = date_range[0] if date_range else min_date
        end_date = date_range[0] if date_range else max_date

    media_only = st.sidebar.checkbox("Posts with Media Only")

    # Create filter dictionary
    filters = {
        'platform': selected_platform if selected_platform != "All" else None,
        'language': selected_language if selected_language != "All" else None,
        'start_date': datetime.datetime.combine(start_date, datetime.time.min),
        'end_date': datetime.datetime.combine(end_date, datetime.time.max),
        'media_only': media_only
    }

    return filters


def platform_analysis_tab(df):
    """Tab content for platform distribution analysis"""
    st.header("Platform Distribution Analysis")

    if df.empty:
        st.warning("No data available for analysis.")
        return

    # Platform distribution
    platform_stats = analyze_platform_distribution(df)

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("Platform Distribution")
        fig = px.pie(
            platform_stats,
            values='Count',
            names='Platform',
            title='Content Distribution by Platform'
        )
        st.plotly_chart(fig)

    with col2:
        st.subheader("Platform Statistics")
        st.dataframe(platform_stats)

    # Platform activity over time
    st.subheader("Platform Activity Timeline")

    # Group posts by date and platform
    df['date'] = pd.to_datetime(df['timestamp'], unit='s').dt.date
    platform_timeline = df.groupby(['date', 'platform']).size().reset_index(name='post_count')
    platform_timeline['date'] = pd.to_datetime(platform_timeline['date'])

    fig = px.line(
        platform_timeline,
        x='date',
        y='post_count',
        color='platform',
        title='Platform Activity Over Time'
    )
    st.plotly_chart(fig)

    # Top authors by platform
    st.subheader("Top Authors by Platform")

    # Group by platform and author, count posts
    platform_authors = df.groupby(['platform', 'author']).size().reset_index(name='post_count')

    # Get top 5 authors for each platform
    top_platform_authors = platform_authors.sort_values(['platform', 'post_count'], ascending=[True, False])
    top_platform_authors = top_platform_authors.groupby('platform').head(5)

    fig = px.bar(
        top_platform_authors,
        x='author',
        y='post_count',
        color='platform',
        title='Top 5 Authors by Platform',
        barmode='group'
    )
    st.plotly_chart(fig)


def media_analysis_tab(df):
    """Tab content for media usage analysis"""
    st.header("Media and Visual Content Analysis")

    if df.empty:
        st.warning("No data available for analysis.")
        return

    # Media usage patterns
    media_results = analyze_media_usage(df)

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("Media Usage Overview")
        fig = px.pie(
            media_results['overall'],
            values='Count',
            names='Media Included',
            title='Posts with Media vs. Text-only',
            color_discrete_map={0: 'grey', 1: 'blue'}
        )
        st.plotly_chart(fig)

    with col2:
        st.subheader("Media Usage by Platform")
        fig = px.bar(
            media_results['by_platform'],
            x='platform',
            y='Percentage',
            title='Percentage of Posts with Media by Platform'
        )
        st.plotly_chart(fig)

    # Top media-sharing authors
    st.subheader("Top Media-Sharing Authors")
    top_media_authors = media_results['by_author'].sort_values('Media Percentage', ascending=False).head(10)

    fig = px.bar(
        top_media_authors,
        x='Author',
        y='Media Percentage',
        title='Top 10 Authors by Media Usage (%)',
        color='Media Percentage',
        color_continuous_scale='Viridis'
    )
    st.plotly_chart(fig)

    # Potential meme detection notification
    st.info("Advanced meme detection requires image download and analysis capabilities, which are demonstrated in concept here but would need integration with image processing APIs for full functionality.")

    # Display media sharing patterns over time
    st.subheader("Media Sharing Timeline")

    # Group media posts by date
    df['date'] = pd.to_datetime(df['timestamp'], unit='s').dt.date
    media_timeline = df.groupby(['date', 'media_included']).size().reset_index(name='post_count')
    media_timeline['date'] = pd.to_datetime(media_timeline['date'])
    media_timeline['media_type'] = media_timeline['media_included'].apply(lambda x: 'With Media' if x == 1 else 'Text Only')

    fig = px.line(
        media_timeline,
        x='date',
        y='post_count',
        color='media_type',
        title='Media vs. Text-only Posts Over Time'
    )
    st.plotly_chart(fig)

def targeting_analysis_tab(df, topic_results, language_results, time_patterns):
    """Tab content for audience targeting analysis"""
    st.header("Audience Targeting Analysis")

    if df.empty:
        st.warning("No data available for analysis.")
        return

    # Detect potential audience targeting
    targeting_indicators = detect_audience_targeting(df, topic_results, language_results, time_patterns)

    # Display targeting overview
    st.subheader("Potential Targeting Indicators")

    if not targeting_indicators:
        st.info("No clear audience targeting indicators detected.")
    else:
        for i, indicator in enumerate(targeting_indicators):
            with st.expander(f"{indicator['type'].capitalize()} Targeting: {indicator['description']}"):
                st.write(f"**Type:** {indicator['type'].capitalize()}")
                st.write(f"**Description:** {indicator['description']}")
                st.write(f"**Confidence:** {indicator['confidence'].capitalize()}")
                st.json(indicator['evidence'])

    # Language analysis
    st.subheader("Language Distribution")

    if not language_results['overall'].empty:
        fig = px.pie(
            language_results['overall'],
            values='Count',
            names='Language',
            title='Content Distribution by Language'
        )
        st.plotly_chart(fig)

        # Language by platform
        if not language_results['by_platform'].empty:
            st.subheader("Language Distribution by Platform")

            pivot_lang = language_results['by_platform'].pivot_table(
                index='platform',
                columns='language',
                values='count',
                fill_value=0
            ).reset_index()

            lang_cols = [col for col in pivot_lang.columns if col != 'platform']

            fig = px.bar(
                pivot_lang,
                x='platform',
                y=lang_cols,
                title='Language Distribution by Platform',
                barmode='stack'
            )
            st.plotly_chart(fig)

    # Posting time patterns
    hour_counts, day_counts, peak_hours, peak_days = time_patterns

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("Posting Time Distribution")

        # Aggregate hour counts across all authors
        hour_agg = hour_counts.groupby('hour')['post_count'].sum().reset_index()

        fig = px.bar(
            hour_agg,
            x='hour',
            y='post_count',
            title='Posts by Hour of Day',
            labels={'hour': 'Hour (24h)', 'post_count': 'Post Count'}
        )
        fig.update_layout(xaxis=dict(tickmode='linear', tick0=0, dtick=1))
        st.plotly_chart(fig)

    with col2:
        st.subheader("Posting Day Distribution")

        # Map day numbers to day names for better readability
        day_map = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday',
                   4: 'Friday', 5: 'Saturday', 6: 'Sunday'}

        # Aggregate day counts across all authors
        day_agg = day_counts.groupby(['day_of_week', 'day_name'])['post_count'].sum().reset_index()
        day_agg = day_agg.sort_values('day_of_week')

        fig = px.bar(
            day_agg,
            x='day_name',
            y='post_count',
            title='Posts by Day of Week',
            category_orders={"day_name": [day_map[i] for i in range(7)]},
            labels={'day_name': 'Day of Week', 'post_count': 'Post Count'}
        )
        st.plotly_chart(fig)

    # Display peak posting times
    st.subheader("Peak Posting Times by Author")

    col1, col2 = st.columns(2)

    with col1:
        st.dataframe(peak_hours)

    with col2:
        st.dataframe(peak_days)

    # Topic targeting analysis
    st.subheader("Topic-Based Targeting")

    if 'author_topics' in topic_results and not topic_results['author_topics'].empty:
        # Get polarizing/targeting topics
        polarizing_topics = ['election', 'vote', 'immigrant', 'refugee', 'government', 'scandal',
                             'protest', 'conspiracy', 'crisis', 'climate', 'vaccine', 'pandemic']

        # Filter for potentially polarizing topics
        polarizing_df = topic_results['author_topics'][
            topic_results['author_topics']['Topic'].str.contains('|'.join(polarizing_topics), case=False)
        ]

        if not polarizing_df.empty:
            st.write("Potentially polarizing topics detected in the content:")

            # Group by topic and count authors
            topic_authors = polarizing_df.groupby('Topic')['Author'].nunique().reset_index()
            topic_authors.columns = ['Topic', 'Author Count']
            topic_authors = topic_authors.sort_values('Author Count', ascending=False)

            fig = px.bar(
                topic_authors,
                x='Topic',
                y='Author Count',
                title='Potentially Polarizing Topics by Number of Authors',
                color='Author Count',
                color_continuous_scale='Viridis'
            )
            st.plotly_chart(fig)

            # Display top authors for each polarizing topic
            st.write("Top authors for each polarizing topic:")
            for topic in topic_authors['Topic'].head(5):
                with st.expander(f"Authors discussing: {topic}"):
                    topic_data = polarizing_df[polarizing_df['Topic'] == topic]
                    topic_author_counts = topic_data.groupby('Author')['Importance'].mean().reset_index()
                    topic_author_counts = topic_author_counts.sort_values('Importance', ascending=False)
                    st.dataframe(topic_author_counts)
        else:
            st.info("No clearly polarizing topics detected.")
    else:
        st.info("No topic data available for targeting analysis.")

    # Temporal targeting patterns
    st.subheader("Temporal Targeting Patterns")

    # Analyze if posts are concentrated during specific times
    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')
    df['hour'] = df['datetime'].dt.hour
    df['day_of_week'] = df['datetime'].dt.dayofweek

    # Define time segments
    time_segments = {
        'Early morning (5-8 AM)': (5, 8),
        'Morning (9-11 AM)': (9, 11),
        'Lunch (12-2 PM)': (12, 14),
        'Afternoon (3-5 PM)': (15, 17),
        'Evening (6-8 PM)': (18, 20),
        'Night (9-11 PM)': (21, 23),
        'Late night (0-4 AM)': (0, 4)
    }

    # Count posts by time segment
    segment_counts = []
    for segment_name, (start, end) in time_segments.items():
        count = df[(df['hour'] >= start) & (df['hour'] <= end)].shape[0]
        segment_counts.append({
            'Segment': segment_name,
            'Count': count,
            'Percentage': (count / len(df) * 100) if len(df) > 0 else 0
        })

    segment_df = pd.DataFrame(segment_counts)

    if not segment_df.empty:
        # Plot time segment distribution
        fig = px.bar(
            segment_df,
            x='Segment',
            y='Percentage',
            title='Post Distribution by Time Segment',
            color='Percentage',
            color_continuous_scale='Viridis',
            labels={'Percentage': 'Percentage of Posts (%)'}
        )
        st.plotly_chart(fig)

        # Check for targeting of specific time segments
        max_segment = segment_df.loc[segment_df['Percentage'].idxmax()]
        if max_segment['Percentage'] > 30:  # If more than 30% in one segment
            st.warning(f"Potential temporal targeting detected: {max_segment['Percentage']:.1f}% of posts are concentrated in the '{max_segment['Segment']}' time segment.")

    # Analyze weekend vs weekday patterns
    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)
    weekend_count = df[df['is_weekend'] == 1].shape[0]
    weekday_count = df[df['is_weekend'] == 0].shape[0]

    weekend_pct = (weekend_count / len(df) * 100) if len(df) > 0 else 0
    weekday_pct = (weekday_count / len(df) * 100) if len(df) > 0 else 0

    # Create weekend vs weekday comparison
    day_type_df = pd.DataFrame([
        {'Type': 'Weekend', 'Count': weekend_count, 'Percentage': weekend_pct},
        {'Type': 'Weekday', 'Count': weekday_count, 'Percentage': weekday_pct}
    ])

    fig = px.pie(
        day_type_df,
        values='Count',
        names='Type',
        title='Weekend vs Weekday Distribution',
        color_discrete_map={'Weekend': 'rgb(102, 197, 204)', 'Weekday': 'rgb(246, 207, 113)'}
    )
    st.plotly_chart(fig)

    # Check if posts are disproportionately on weekends
    if weekend_pct > 40:  # Weekends are only 2/7 days (28.6%)
        st.warning(f"Potential weekend targeting detected: {weekend_pct:.1f}% of posts occur on weekends, which is disproportionately high.")

    # Return targeting indicators for use in other tabs
    return targeting_indicators


def coordination_analysis_tab(df, content_sim_df, temporal_windows, url_patterns):
    """Tab content for coordination detection analysis"""
    st.header("Coordination Detection Analysis")

    if df.empty:
        st.warning("No data available for analysis.")
        return

    # Network analysis of coordinated accounts
    network_results = identify_coordination_networks(df, content_sim_df, temporal_windows, url_patterns)

    # Display network summary
    st.subheader("Coordination Network Summary")

    # Display network metrics
    metrics = network_results['metrics']

    col1, col2, col3 = st.columns(3)
    col1.metric("Coordinated Authors", metrics['coordinated_authors'])
    col2.metric("Coordination Clusters", metrics['total_clusters'])
    col3.metric("Largest Cluster Size", metrics['largest_cluster_size'])

    # Network visualization
    st.subheader("Coordination Network Visualization")
    network_fig = generate_network_visualization(network_results)
    if network_fig:
        st.plotly_chart(network_fig)
    else:
        st.info("No coordination network detected or insufficient data for visualization.")

    # Timeline visualization
    st.subheader("Coordination Timeline")
    timeline_fig = generate_timeline_visualization(df, temporal_windows)
    if timeline_fig:
        st.plotly_chart(timeline_fig)
    else:
        st.info("Insufficient temporal data for timeline visualization.")

    # Display detected coordination clusters
    st.subheader("Detected Coordination Clusters")

    if 'clusters' in network_results and network_results['clusters']:
        for i, cluster in enumerate(network_results['clusters']):
            with st.expander(f"Cluster {cluster['cluster_id']} ({cluster['size']} authors)"):
                st.write(f"**Cluster Size:** {cluster['size']} authors")
                st.write(f"**Network Density:** {cluster['density']:.4f}")
                st.write("**Members:**")
                st.write(", ".join(cluster['members']))
    else:
        st.info("No coordination clusters detected.")

    # Temporal coordination patterns
    st.subheader("Temporal Coordination Patterns")

    if not temporal_windows.empty:
        st.dataframe(temporal_windows)
    else:
        st.info("No temporal coordination patterns detected.")

def campaign_analysis_tab(df, temporal_windows):
    """Tab content for campaign goal analysis"""
    st.header("Campaign Goal Analysis")

    if df.empty or temporal_windows.empty:
        st.warning("No data available for campaign analysis.")
        return

    # Detect campaign pattern types
    pattern_types = detect_coordination_pattern_types(df, temporal_windows)

    # Display pattern type summary
    st.subheader("Campaign Goal Assessment")

    col1, col2, col3 = st.columns(3)

    amp_count = len(pattern_types['amplification'])
    dis_count = len(pattern_types['disruption'])
    sup_count = len(pattern_types['suppression'])

    col1.metric("Amplification Patterns", amp_count)
    col2.metric("Disruption Patterns", dis_count)
    col3.metric("Suppression Patterns", sup_count)

    # Create pie chart of pattern types
    pattern_counts = [
        {'Type': 'Amplification', 'Count': amp_count},
        {'Type': 'Disruption', 'Count': dis_count},
        {'Type': 'Suppression', 'Count': sup_count},
        {'Type': 'Unknown', 'Count': len(pattern_types['unknown'])}
    ]

    if sum(item['Count'] for item in pattern_counts) > 0:
        fig = px.pie(
            pattern_counts,
            values='Count',
            names='Type',
            title='Distribution of Campaign Pattern Types'
        )
        st.plotly_chart(fig)

    # Display pattern details
    for pattern_type in ['amplification', 'disruption', 'suppression']:
        if not pattern_types[pattern_type].empty:
            with st.expander(f"{pattern_type.capitalize()} Patterns ({len(pattern_types[pattern_type])})"):
                st.dataframe(pattern_types[pattern_type])

    # Campaign scale estimation
    st.subheader("Campaign Scale Estimation")

    # Get URL patterns and network results for scale estimation
    url_patterns = analyze_url_patterns(df)
    content_sim_df = analyze_content_similarity(df)
    network_results = identify_coordination_networks(df, content_sim_df, temporal_windows, url_patterns)

    scale_assessment = estimate_operation_scale(network_results, temporal_windows, url_patterns)

    st.write(f"**Assessed Scale:** {scale_assessment['scale'].capitalize()}")
    st.write(f"**Confidence:** {scale_assessment['confidence'].capitalize()}")
    st.write(f"**Assessment:** {scale_assessment['description']}")

    with st.expander("Scale Assessment Metrics"):
        st.json(scale_assessment['metrics'])

def topic_analysis_tab(df):
    """Tab content for topic analysis"""
    st.header("Topic Analysis")

    if df.empty:
        st.warning("No data available for topic analysis.")
        return

    # Analyze topics
    topic_results = analyze_topic_patterns(df)

    # Display overall top topics
    st.subheader("Top Topics Across All Content")

    if not topic_results['top_topics'].empty:
        # Create bar chart of top topics
        fig = px.bar(
            topic_results['top_topics'].head(15),
            x='Topic',
            y='Importance',
            title='Top 15 Topics by Importance'
        )
        st.plotly_chart(fig)

        # Word cloud
        if not topic_results['top_topics'].empty:
            st.subheader("Topic Word Cloud")

            topic_dict = dict(zip(topic_results['top_topics']['Topic'], topic_results['top_topics']['Importance']))

            # Generate word cloud
            wordcloud = WordCloud(
                width=800,
                height=400,
                background_color='white',
                colormap='viridis',
                max_words=100
            ).generate_from_frequencies(topic_dict)

            # Display word cloud
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.imshow(wordcloud, interpolation='bilinear')
            ax.axis('off')
            st.pyplot(fig)
    else:
        st.info("No topic data available.")

    # Topics by platform
    st.subheader("Top Topics by Platform")

    if not topic_results['platform_topics'].empty:
        # Group by platform
        platforms = topic_results['platform_topics']['Platform'].unique()

        for platform in platforms:
            with st.expander(f"Topics on {platform}"):
                platform_data = topic_results['platform_topics'][
                    topic_results['platform_topics']['Platform'] == platform
                ].sort_values('Importance', ascending=False).head(10)

                fig = px.bar(
                    platform_data,
                    x='Topic',
                    y='Importance',
                    title=f'Top 10 Topics on {platform}'
                )
                st.plotly_chart(fig)
    else:
        st.info("No platform topic data available.")

    # Topics by author
    st.subheader("Author Topic Analysis")

    if not topic_results['author_topics'].empty:
        # Get top authors by topic count
        top_authors = topic_results['author_topics']['Author'].value_counts().head(5).index.tolist()

        for author in top_authors:
            with st.expander(f"Topics for {author}"):
                author_data = topic_results['author_topics'][
                    topic_results['author_topics']['Author'] == author
                ].sort_values('Importance', ascending=False).head(10)

                fig = px.bar(
                    author_data,
                    x='Topic',
                    y='Importance',
                    title=f'Top 10 Topics for {author}'
                )
                st.plotly_chart(fig)
    else:
        st.info("No author topic data available.")

    return topic_results

def insights_report_tab(df, network_results, temporal_windows, scale_assessment, targeting_indicators):
    """Tab for generating insights and reports"""
    st.header("Insights and Reports")

    if df.empty:
        st.warning("No data available for insights.")
        return

    # Overview insights
    st.subheader("Key Insights")

    # Platform insights
    platforms = df['platform'].unique()
    platform_str = ", ".join(platforms)
    st.write(f"**Platforms Used:** {platform_str}")

    # Media usage
    media_percentage = (df['media_included'].sum() / len(df) * 100).round(2)
    st.write(f"**Media Usage:** {media_percentage}% of posts contain media references")

    # Coordination insights
    if 'metrics' in network_results:
        metrics = network_results['metrics']

        if metrics['coordinated_authors'] > 0:
            st.write(f"**Coordination Detected:** Yes, involving {metrics['coordinated_authors']} authors in {metrics['total_clusters']} clusters")
        else:
            st.write("**Coordination Detected:** No significant coordination patterns detected")

    # Scale assessment
    st.write(f"**Operation Scale:** {scale_assessment['scale'].capitalize()} ({scale_assessment['confidence'].capitalize()} confidence)")

    # Targeting insights
    if targeting_indicators:
        targeting_types = [indicator['type'] for indicator in targeting_indicators]
        targeting_str = ", ".join(set(targeting_types))
        st.write(f"**Audience Targeting:** Detected ({targeting_str})")
    else:
        st.write("**Audience Targeting:** No clear targeting patterns detected")

    # Campaign goals (if available)
    if not temporal_windows.empty:
        pattern_types = detect_coordination_pattern_types(df, temporal_windows)

        # Count pattern types
        pattern_counts = {
            'amplification': len(pattern_types['amplification']),
            'disruption': len(pattern_types['disruption']),
            'suppression': len(pattern_types['suppression'])
        }

        # Determine dominant pattern
        dominant_pattern = max(pattern_counts.items(), key=lambda x: x[1])

        if dominant_pattern[1] > 0:
            st.write(f"**Likely Campaign Goal:** {dominant_pattern[0].capitalize()}")
        else:
            st.write("**Likely Campaign Goal:** Insufficient data to determine")

    # Generate full report
    st.subheader("Generate Full Report")

    if st.button("Generate Detailed PDF Report"):
        # This would be a placeholder for actual PDF generation
        # In a real implementation, you would use a library like reportlab or weasyprint

        st.info("PDF report generation would be implemented here.")

        # Display report generation options in the future
        with st.expander("Report Generation Options"):
            st.checkbox("Include network visualizations", value=True)
            st.checkbox("Include topic analysis", value=True)
            st.checkbox("Include temporal patterns", value=True)
            st.checkbox("Include raw data tables", value=False)

    # Export data options
    st.subheader("Export Analysis Data")

    export_format = st.selectbox("Export Format", ["CSV", "Excel", "JSON"])

    if st.button("Export Data"):
        # This would be a placeholder for actual export functionality
        # In a real implementation, you would generate the file and provide a download link

        st.info(f"Data export in {export_format} format would be implemented here.")

def main():
    """Main application function"""
    # Initialize database connection
    conn = upload_data_component()

    # Show filter options if data is available
    if conn is not None:
        filters = filter_options_component(conn)

        if filters is not None:
            # Load data with filters
            df = load_from_database(conn, filters)

            if df.empty:
                st.warning("No data matches the selected filters. Please adjust your filters.")
            else:
                # Display dataset overview
                st.header("Dataset Overview")
                col1, col2, col3, col4 = st.columns(4)
                col1.metric("Total Posts", len(df))
                col2.metric("Unique Authors", df['author'].nunique())
                col3.metric("Platforms", df['platform'].nunique())
                col4.metric("Date Range", f"{df['datetime'].min().date()} to {df['datetime'].max().date()}")

                # Create tabs for different analysis components
                tabs = st.tabs([
                    "Platform Analysis",
                    "Media Analysis",
                    "Coordination Detection",
                    "Campaign Goals",
                    "Topic Analysis",
                    "Audience Targeting",
                    "Insights & Reports"
                ])

                # Run analyses
                with st.spinner('Running analysis...'):
                    # Content similarity analysis
                    content_sim_df = analyze_content_similarity(df)

                    # URL pattern analysis
                    url_patterns = analyze_url_patterns(df)

                    # Time pattern analysis
                    time_patterns = analyze_time_patterns(df)

                    # Temporal coordination detection
                    temporal_windows = detect_temporal_patterns(df)

                    # Language analysis
                    language_results = analyze_language_patterns(df)

                    # Topic analysis
                    topic_results = analyze_topic_patterns(df)

                    # Network analysis for insights
                    network_results = identify_coordination_networks(df, content_sim_df, temporal_windows, url_patterns)

                    # Scale assessment
                    scale_assessment = estimate_operation_scale(network_results, temporal_windows, url_patterns)

                    # Audience targeting analysis
                    targeting_indicators = detect_audience_targeting(df, topic_results, language_results, time_patterns)

                # Platform Analysis tab
                with tabs[0]:
                    platform_analysis_tab(df)

                # Media Analysis tab
                with tabs[1]:
                    media_analysis_tab(df)

                # Coordination Detection tab
                with tabs[2]:
                    coordination_analysis_tab(df, content_sim_df, temporal_windows, url_patterns)

                # Campaign Goals tab
                with tabs[3]:
                    campaign_analysis_tab(df, temporal_windows)

                # Topic Analysis tab
                with tabs[4]:
                    topic_analysis_tab(df)

                # Audience Targeting tab
                with tabs[5]:
                    targeting_analysis_tab(df, topic_results, language_results, time_patterns)

                # Insights & Reports tab
                with tabs[6]:
                    insights_report_tab(df, network_results, temporal_windows, scale_assessment, targeting_indicators)

    # Add footer with tool information
    st.markdown("""
    ---
    ### About this Tool
    This dashboard is designed to analyze social media data for potential coordination patterns and inauthentic behavior.
    It can help identify coordinated campaigns across multiple platforms and provide insights into campaign strategies and goals.

    #### Features:
    - Multi-platform analysis
    - Media usage detection
    - Coordination network visualization
    - Campaign goal assessment
    - Audience targeting analysis
    - Topic trend identification
    """)

if __name__ == "__main__":
    main()
